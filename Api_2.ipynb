{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Calling OpenAI API with deployment: gpt-4o, Retry: 1\n",
      "INFO:__main__:OPENAI CALL MESSAGE: Retry 1 | Func: extract_attributes_from_rd_brief | Duration: 8.841s completion_tokens: 669 | prompt_tokens: 3909 | total_tokens: 4578 | completion_tokens_details: {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0} | prompt_tokens_details: {'audio_tokens': 0, 'cached_tokens': 0} | \n",
      "INFO:__main__:Token Usage Summary: {'extract_attributes_from_rd_brief': 4578}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ OpenAI Response:\n",
      " {\n",
      "  \"allergen_items\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {\n",
      "      \"Contains_Milk_Proteins\": {\n",
      "        \"value\": \"No\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Pareve designation implies absence of milk proteins.\"\n",
      "      },\n",
      "      \"Contains_Egg_Products\": {\n",
      "        \"value\": \"No\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Pareve designation implies absence of egg products.\"\n",
      "      },\n",
      "      \"Contains_Soy_Proteins\": {\n",
      "        \"value\": \"No\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Pareve designation implies absence of soy proteins.\"\n",
      "      },\n",
      "      \"Contains_Nuts_and_Almonds\": {\n",
      "        \"value\": \"No\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Pareve designation implies absence of nuts and almonds.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"claims_certifications\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {\n",
      "      \"Kosher_Certificate\": {\n",
      "        \"value\": \"Pareve\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Pareve designation indicates kosher certification.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"ingredients_composition\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {\n",
      "      \"Components_Specifications\": {\n",
      "        \"value\": \"Chocolate chips made from the same recipe as 12M chip, size adjusted to 10M.\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Document specifies recipe consistency between 10M and 12M chips.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"legal_specifications\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {\n",
      "      \"Legal_Denomination\": {\n",
      "        \"value\": \"Chocolate Chips\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Product type and description imply legal denomination.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"nutritional_values\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {}\n",
      "  },\n",
      "  \"packaging_information\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {\n",
      "      \"Packaging_Info\": {\n",
      "        \"value\": \"50 lb. or 30 lb. carton\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Document mentions Kellogg's preference for packaging sizes.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"sales_commercial\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {\n",
      "      \"Minimum_Order_Quantity_In_UoM\": {\n",
      "        \"value\": \"50 lb. or 30 lb. carton\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Document suggests new SKU creation with specified carton sizes.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"technical_specifications\": {\n",
      "    \"explicit\": {},\n",
      "    \"inferred\": {\n",
      "      \"Dimensions_Count_lb_From\": {\n",
      "        \"value\": \"30 lb.\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Document mentions 30 lb. carton as a potential packaging option.\"\n",
      "      },\n",
      "      \"Dimensions_Count_lb_To\": {\n",
      "        \"value\": \"50 lb.\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Document mentions 50 lb. carton as a potential packaging option.\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "‚úÖ Extracted attributes saved to 'e_a_00147036.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ------------------ Load environment variables ------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------ Logging setup ------------------\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# ------------------ Azure Credentials ------------------\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=os.getenv(\"AZURE_TENANT_ID\"),\n",
    "    client_id=os.getenv(\"AZURE_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"AZURE_CLIENT_SECRET\"),\n",
    ")\n",
    "\n",
    "# ------------------ Globals ------------------\n",
    "TOKEN_USAGE = {}\n",
    "TOTAL_API_CALL = 0\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# ------------------ Azure Document Intelligence Client ------------------\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_KEY = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "if not AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT or not AZURE_DOCUMENT_INTELLIGENCE_KEY:\n",
    "    raise EnvironmentError(\"Missing Azure Document Intelligence credentials\")\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT,\n",
    "    credential=AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)\n",
    ")\n",
    "\n",
    "# ------------------ OCR Function ------------------\n",
    "def process_file_new_ocr(file_path: str) -> List[Dict]:\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    logging.info(f\"Processing file: {file_path.name}\")\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        document_bytes = f.read()\n",
    "\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-read\",\n",
    "        body=document_bytes,\n",
    "    )\n",
    "    result = poller.result()\n",
    "\n",
    "    attachment_list = []\n",
    "    for page in result.pages:\n",
    "        text = \" \".join([line.content for line in page.lines])\n",
    "        attachment_list.append({\n",
    "            \"title\": file_path.name,\n",
    "            \"pagenum\": page.page_number,\n",
    "            \"content\": text\n",
    "        })\n",
    "\n",
    "    return attachment_list\n",
    "\n",
    "# ------------------ Azure OpenAI LLM Clients ------------------\n",
    "AzureChatOpenAI.model_rebuild()\n",
    "\n",
    "def llm():\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=AZURE_OPENAI_DEPLOYMENT,\n",
    "        api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        openai_api_key=access_token,\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    )\n",
    "\n",
    "def openai_llm():\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    return AzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "        api_key=access_token,\n",
    "    )\n",
    "\n",
    "def embeddings():\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    return AzureOpenAIEmbeddings(\n",
    "        model=os.getenv(\"EMBEDDING_AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        azure_endpoint=os.getenv(\"EMBEDDING_AZURE_OPENAI_ENDPOINT\"),\n",
    "        openai_api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "        api_key=access_token,\n",
    "    )\n",
    "\n",
    "# ------------------ OpenAI Call Utilities ------------------\n",
    "def add_token_usage_logs(llm_output, message=\"\"):\n",
    "    token_usage_string = \"\"\n",
    "    for key, value in llm_output.to_dict().get(\"usage\", {}).items():\n",
    "        token_usage_string += f\"{key}: {value} | \"\n",
    "    logger.info(f\"{message} {token_usage_string}\")\n",
    "    return llm_output.to_dict().get(\"usage\", {}).get(\"total_tokens\", 0)\n",
    "\n",
    "def openai_call(sys_prompt, prompt_struc, deployment_name=AZURE_OPENAI_DEPLOYMENT, additional_message=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_struc},\n",
    "    ]\n",
    "    return _call_openai(messages, deployment_name, additional_message)\n",
    "\n",
    "def _call_openai(messages, deployment_name, additional_message):\n",
    "    global TOTAL_API_CALL\n",
    "    max_retries = 5\n",
    "    for current_retry in range(max_retries):\n",
    "        try:\n",
    "            time.sleep(5)\n",
    "            logger.info(f\"Calling OpenAI API with deployment: {deployment_name}, Retry: {current_retry + 1}\")\n",
    "            start_time = time.time()\n",
    "            response = openai_llm().chat.completions.create(\n",
    "                model=deployment_name,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            TOTAL_API_CALL += 1\n",
    "            duration = round(time.time() - start_time, 3)\n",
    "            output = response.choices[0].message.content\n",
    "            if output:\n",
    "                msg = f\"OPENAI CALL MESSAGE: Retry {current_retry + 1} | Func: {additional_message} | Duration: {duration}s\"\n",
    "                total_tokens = add_token_usage_logs(response, message=msg)\n",
    "                TOKEN_USAGE[additional_message] = TOKEN_USAGE.get(additional_message, 0) + total_tokens\n",
    "                logger.info(f\"Token Usage Summary: {TOKEN_USAGE}\")\n",
    "                print(\"\\nüîπ OpenAI Response:\\n\", output)\n",
    "                return output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI Call Error: {e}\")\n",
    "    logger.warning(\"Maximum retries reached. No response returned.\")\n",
    "    return None\n",
    "\n",
    "# ------------------ MAIN EXECUTION BLOCK ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    test_file = \"R&D Supplier Brief - Chocolate Coating for Murray Street.pdf\"\n",
    "\n",
    "    try:\n",
    "        ocr_pages = process_file_new_ocr(test_file)\n",
    "        extracted_text = \"\\n\".join([p[\"content\"] for p in ocr_pages])\n",
    "\n",
    "        system_prompt = \"\"\"\n",
    "You are a domain-aware assistant that extracts structured product development attributes from customer R&D or supplier brief documents.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Your Role:\n",
    "You are interpreting a **customer brief** ‚Äî a document that includes both direct requirements and contextual information. Your job is to:\n",
    "1. Extract **explicit** attributes: those that are **directly requested by the customer** in the brief.\n",
    "2. Extract **inferred** attributes: high-confidence values **logically implied** by the brief‚Äôs context, product type, or regulatory standards, with a supporting `\"note\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Mandatory Instructions:\n",
    "- Maximize the number of attributes extracted from the brief.\n",
    "- **Always extract** the following fields if present ‚Äî even if not explicitly requested:\n",
    "\n",
    "['Material Code', 'Legislation', 'Cluster', 'Cluster_Label', 'Legal_Denomination', 'Legislation_Description', 'Min_Dry_Cocoa_Solids', 'Dry_Milk_Solids', 'MilkFat', 'Dry_Fat_Free_Cocoa_Solids', 'Typical_Chocolate_Liquor', 'Typical_Cocoa_Content', 'Total_Legal_Fat_Content', 'MilkFat_Chocolate_Part', 'Dry_Milk_Solids_On_Total_Production', 'Dry_Milk_Solids_Chocolate_Part', 'Typical_Nonfat_Milk_Solids', 'Typ_Nonfat_Cocoa_Sol_Choc_Part', 'Sum_Dry_Cocoa_And_Milk_Solids', 'Cocoa_Butter_Content', 'Alkalizing_Agent_K2CO3_DFFCS', 'Component', 'Item', 'Level', 'Material_Group', 'Material_Type', 'Parent_Material', 'Parent_Material_Label', 'Source_Generated_Field', 'components_Specifications', 'Sugars_g', 'Salt_g', 'Trans_Fatty_Acid_TFa_g', 'Energy_Value_Kcal', 'Energy_Value_Kj', 'Protein_g', 'Protein_DV_perc', 'Total_Carbohydrates_g', 'Total_Carbohydrates_DV_perc', 'Saturated_Fatty_Acid_g', 'Calories_From_Fat', 'Cholesterol_mg', 'Cholesterol_DV_perc', 'Total_Fat_DV_perc', 'Fibre_g', 'Dietary_Fibre_DV_perc', 'Vitamin_A_mcg', 'Vitamin_C_mg', 'Sodium_mg', 'Sodium_DV_perc', 'Iron_mg', 'Calcium_mg', 'Available_Carbohydrates_g', 'Total_Fat_g', 'Allergen_Statements', 'Contains_Milk_Proteins', 'Contains_Egg_Products', 'Contains_Soy_Proteins', 'Contains_Wheat', 'Contains_Rye', 'Contains_Fish', 'Contains_Crustacean_And_Shell_Fish', 'Contains_Hazelnuts_Almonds', 'Contains_Peanuts', 'Contains_Sulphite_E220_E227', 'Contains_Celery', 'Contains_Sesame_Products', 'Suitable_For_Vegetarians', 'Suitable_For_Vegans', 'Hazelnut_Oil_Almond_Oil', 'Contains_Sesame_Oil', 'Contains_Peanut_Oil', 'Contains_Mustard', 'Contains_Molluscs', 'Contains_Lupin', 'Contains_Buckwheat', 'Plant_BoM_Owner_Short', 'Underlying_Liquid', 'Dimensions_Vibration_Drops_EU_Short', 'Project_Number_Short', 'Dimensions_Production_Tools_Us', 'Material_Description', 'Base_Type', 'Moulding_Type', 'Product_Type', 'Colour_TF', 'Project_Manager', 'Dimensions_Vibration_Drops_EU', 'Project_Phase', 'Certification', 'Base_Colour', 'Additional_Colour', 'Kosher_Certificate', 'Country_Claim', 'Plant_BoM_Owner', 'Type_3_Short', 'Dosage_Per_200ml_Cold_Milk', 'Dosage_Per_200ml_Cold_Water', 'Dosage_Per_200ml_Hot_Milk', 'Dosage_Per_200Ml_Hot_Water', 'Fineness_Type', 'Colour_L_Value_From', 'pH_From', 'pH_To', 'Normalised_Linear_Mpa_S_From', 'Normalised_Linear_Viscosity_mPaS_To', 'Normalised_Casson_Mpa_S_From', 'Normalised_Casson_Mpa_S_To', 'Normalised_Yield_Pa_From', 'Normalised_Yield_Pa_To', 'Fineness_Micrometer_From', 'Fineness_Micrometer_To', 'Dimensions_Length_From', 'Dimensions_Length_To', 'Dimensions_Width_From', 'Dimensions_Width_To', 'Dimensions_Height_From', 'Dimensions_Count_Kg_From', 'Dimensions_Count_Kg_To', 'Dimensions_Sieve_Fraction_From', 'Dimensions_Sieve_Fraction_To', 'Protein', 'Dimensions_Count_lb_From', 'Dimensions_Count_lb_To', 'Brookfield_40C_S27_20_RPM_From', 'Brookfield_40C_S27_20_RPM_To', 'Brookfield_50C_S27_20_RPM_From', 'Brookfield_50C_S27_20_RPM_To', 'Brookfield_50C_S27_Regression_From', 'Brookfield_50C_S27_Regression_To', 'Brookfield_50C_S27_Yield_From', 'Brookfield_50C_S27_Yield_To', 'Brookfield_40C_S27_Regression_From', 'Brookfield_40C_S27_Regression_To', 'Brookfield_40C_S27_Yield_From', 'Brookfield_40C_S27_Yield_To', 'Water_Activity_From', 'Water_Activity_To', 'Shelflife', 'Bulk_Density_Untapped_From', 'Bulk_Density_Untapped_To', 'Bulk_Density_Tapped_x100_From', 'Bulk_Density_Tapped_x100_To', 'Dosage_Test_Grams_From', 'Dosage_Test_Grams_To', 'Material_Group_Short', 'Packaging_Info', 'Sales_Organisation', 'Plant_Where_Produced_OR_Available', 'Primary_Weight_Unit', 'Primary_Count_Unit', 'Material_Group_Long', 'Brand', 'Kosher_recipe_not_certificate_', 'Marking', 'Primary_weight', 'Primary_Count', 'NGM_Status', 'Regional_Supply_Policy_West_Europe', 'Additional_Premium_Group', 'Regional_Speciality_Category_West_Europe', 'Regional_Speciality_Category_East_Europe', 'Regional_Speciality_Category_US', 'Regional_Speciality_Category_Asia', 'Regional_Supply_Policy_US', 'Mass_Balance_Certification', 'Western_EU_BC_Selection', 'Regional_Sales_Forecast_West_Europe', 'Regional_Sales_Forecast_East_Europe', 'Regional_Sales_Forecast_US', 'Regional_Sales_Forecast_Asia', 'Regional_Premium_Category_West_Europe', 'Regional_Premium_Category_East_Europe', 'Regional_Premium_Category_US', 'Regional_Premium_Category_Asia', 'Product_Category_West_Europe_', 'Product_Category_East_Europe', 'Product_Category_US', 'Product_Category_Asia', 'Eastern_EU_BC_Selection', 'Standard_Range_Mexico', 'Standard_Range_US', 'Commercial_Name', 'Commodity_Code', 'Lifecycle_status', 'Distribution_Channel', 'Calculated_Price_Currency', 'Sales_Organisation_Distribution_Channel', 'Delivery_Unit_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_In_UoM', 'Delivery_unit_Qty_In_UoM', 'Replenishment_Lead_Time', 'Sales_Last_12_Months_North_America_in_KG', 'Sales_Last_12_Months_Asia_in_KG_', 'Sales_Last_12_Months_EEMEA_in_KG_', 'Sales_Last_12_Months_West_Europe_in_KG', 'Sales_Last_12_Months_South_America_in_KG', 'Calculated_Price', 'Sales_Last_12_Months_Total_In_Kg', 'Contains_Hydrogenated', 'Fat', 'Polyols', 'Nuts_and_Almonds', 'Total_Fat_On_Spec_perc_From', 'Total_Fat_On_Spec_perc_To', 'Hydrogenated', 'Core_OR_Extended', 'Core_Region', 'Core_Country', 'Core_Segment', 'Core_Subsegment', 'Category', 'Region', 'Customer_Dedication', 'Proj_Phase', 'Pack_Code', 'Base_Unit_Of_Measure', 'Smallest_Unit_Weight_In_Kg', 'Sample_Unit', 'Units_Per_Layer', 'Units_Per_Pallet', 'Pallet', 'Pallet_Net_Weight_In_Kg', 'Pallet_Gross_Weight_In_Kg', 'Length', 'Width', 'Height', 'Pallet_Type', 'Sales_Unit', 'Layer', 'Delivery_Unit_AUM', 'Certification_Tag', 'Colour_Type_Tag', 'Flavor_Type_Tag', 'Ingredients_Tag']\n",
    "\n",
    "- You must read the data at the **Material Code + Legislation** level, since granularity is based on these two fields.\n",
    "\n",
    "- Regardless of how they appear in the brief, **always extract** the following attributes:\n",
    "  - `Product_Type`\n",
    "  - `Base_Type`\n",
    "  - `Moulding_Type`\n",
    "  - `Components_Specifications` like sugar, milk ingredients, cocoa butter, natural vanilla extract, emulsifier etc\n",
    "  - `Fat content`\n",
    "  - `pH details`\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è Output Categories:\n",
    "Group extracted attributes under the following **8 standard categories**, each containing:\n",
    "- `\"explicit\"`: only those clearly **requested by the customer**\n",
    "- `\"inferred\"`: high-confidence deductions with `\"note\"`\n",
    "\n",
    "1. allergen_items  \n",
    "2. claims_certifications  \n",
    "3. ingredients_composition  \n",
    "4. legal_specifications  \n",
    "5. nutritional_values  \n",
    "6. packaging_information  \n",
    "7. sales_commercial  \n",
    "8. technical_specifications\n",
    "\n",
    "---\n",
    "\n",
    "### Also include **explicit attributes based on product-type logic**:\n",
    "\n",
    "#### Chocolate Type-specific (Dark / Milk / White):\n",
    "- Total fat (% or g/100g)\n",
    "- Minimum dry cocoa solids (%)\n",
    "- Dry fat-free cocoa solids (%)\n",
    "- Milkfat (%) ‚Äì for milk/white chocolate\n",
    "- Dry milk solids (%)\n",
    "- Fineness type (e.g., FP or micrometer)\n",
    "- Norm linear viscosity (mPa.s)\n",
    "- Casson viscosity (mPa.s)\n",
    "- Yield value (Pa)\n",
    "\n",
    "#### üç¨ Moulding/Shape Specifics:\n",
    "- Length, Width, Height\n",
    "- Vibration (drops)\n",
    "- Primary Count or Count/Unit\n",
    "- Sieve fraction (if relevant)\n",
    "\n",
    "#### üß™ Compound/Fillings:\n",
    "- Check for ‚Äúcontains hydrogenated‚Äù or hydrogenated fats content\n",
    "\n",
    "#### ü•ú If Nuts are Mentioned:\n",
    "- % of nuts or quantity\n",
    "\n",
    "#### üåç Export Targets (e.g., EU, US, China):\n",
    "- Legal declaration required\n",
    "- Country-specific regulatory compliance\n",
    "- Typical cocoa content\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Inference Guidelines:\n",
    "\n",
    "Use domain knowledge to **infer high-confidence values**:\n",
    "\n",
    "- PGPR or lecithin implies vegetable fats or emulsifiers  \n",
    "- Codex mention ‚Üí infer standard ranges for cocoa, milk solids, and sugar  \n",
    "- PGPR ‚â§ 0.5% ‚Üí infer presence of vegetable fats  \n",
    "- Enrobing/frozen ‚Üí infer ‚ÄúFreezer Stability‚Äù, ‚ÄúSnap Texture‚Äù  \n",
    "- Melting/flow/viscosity mentions ‚Üí infer ‚ÄúFlowability‚Äù  \n",
    "- Export shipping ‚Üí infer ‚ÄúShelf Stability‚Äù  \n",
    "- RSPO, FSC, Rainforest ‚Üí infer sustainable sourcing  \n",
    "- ‚ÄúNon-GMO‚Äù, ‚ÄúNo artificial preservatives/flavors‚Äù ‚Üí infer ‚Äúfree-from‚Äù claims  \n",
    "- Codex, EU, FDA law references ‚Üí infer ‚ÄúLegal Declaration Required‚Äù  \n",
    "- ‚ÄúNo sugar added‚Äù ‚Üí infer ‚ÄúLow Sugar Claim‚Äù  \n",
    "- Box, pouch, bag, sachet ‚Üí infer ‚ÄúPackaging Format‚Äù  \n",
    "- FSC/eco-labels ‚Üí infer ‚ÄúSustainable Packaging‚Äù  \n",
    "- MOQ, volume, pricing ‚Üí infer ‚ÄúSales Channel‚Äù or ‚ÄúIndicative Volume‚Äù  \n",
    "- ‚ÄúMay contain traces‚Ä¶‚Äù ‚Üí infer cross-contamination risk  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Output Rules:\n",
    "- Only include `\"explicit\"` if clearly requested by the customer.\n",
    "- Maximize attribute coverage ‚Äî both **structured fields** and **inferred logic**\n",
    "- Do not Hallucinate and Do **not fabricate** values without strong support.\n",
    "- Return clean **valid JSON** only ‚Äî no markdown, comments, or explanations.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Output Format:\n",
    "Return structured JSON in this format:\n",
    "\n",
    "{\n",
    "  \"category_name\": {\n",
    "    \"explicit\": {\n",
    "      \"Attribute Name\": {\n",
    "        \"value\": \"...\",\n",
    "        \"source\": \"explicit\"\n",
    "      }\n",
    "    },\n",
    "    \"inferred\": {\n",
    "      \"Attribute Name\": {\n",
    "        \"value\": \"...\",\n",
    "        \"source\": \"inferred\",\n",
    "        \"note\": \"brief justification\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "Each `category_name` must match one of the 8 categories above.\n",
    "\"\"\"\n",
    "        user_prompt = f\"\"\"Here is the extracted text from an R&D document:\n",
    "\n",
    "--- START OF DOCUMENT ---\n",
    "First, we need an Opportunity for this project - @Matthew Steinmetz can you please do this for us please?\n",
    "Second, yes, let's send the 10M chips to Aris (CHD-DR-6000329-002) along with a 12M in case he doesn't have it on hand.  The only 12M KParve we have is in superbag so there is no stock at the sample room:  we'll need to send the 12M KDairy CHD-DR-6000315-036.\n",
    "\n",
    "Thank you very much,\n",
    "\n",
    "Marie-Pierre Bolduc \n",
    "R&D Product LifeCycle Manager - America \n",
    "M: 450-230-3643\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On Tue, Feb 7, 2023 at 3:13 PM Stacey Wing <stacey_wing@barry-callebaut.com> wrote:\n",
    "(Removed customer)\n",
    "\n",
    "Please confirm that I am sending 1 lb of CHD-DR-6000329-002. Want to be sure I'm following along correctly! We have product in the sample room, so we can get it to him early next week.\n",
    "\n",
    "\n",
    "On Tue, Feb 7, 2023 at 11:26 AM <choc4sale@aol.com> wrote:\n",
    "Understood Aris. Therefore we will send you a sample of our 10M Pareve chocolate chip for you to experiment with. It will be made from the recipe that the 12M chip is made from -- just the size will change.  best, John\n",
    "\n",
    "\n",
    "-----Original Message-----\n",
    "From: Adams, Animequom <Aris.Adams@kellogg.com>\n",
    "To: choc4sale@aol.com <choc4sale@aol.com>\n",
    "Cc: marie-pierre_bolduc@barry-callebaut.com <marie-pierre_bolduc@barry-callebaut.com>; matthew_steinmetz@barry-callebaut.com <matthew_steinmetz@barry-callebaut.com>; stacey_wing@barry-callebaut.com <stacey_wing@barry-callebaut.com>\n",
    "Sent: Tue, Feb 7, 2023 11:20 am\n",
    "Subject: Re: [EXTERNAL] Quick Question regarding Project Chainsaw and the Original Chocolate Chip PopTart\n",
    "\n",
    "Hi John,\n",
    "\n",
    "The project working on the Chocolate Chip Pop Tart does not have launch timing as of now.  The product can be run using the current 15M item, so there is no urgency on switching it to allergen free.  I am not on the project and this is second hand information mind you, but as I understand, other actions will happen before the chip needs to be swapped to an allergen free chip.\n",
    "\n",
    "I think for now, we need to treat Chainsaw as a standalone project.  We may know more before launch, but it is too early at this point to know when we would need an allergen free chip for CC Pop Tart.\n",
    "\n",
    "Regards,\n",
    "\n",
    "Aris Adams\n",
    "Lead Food Designer, PWS\n",
    "Phone: (269) 366-7169\n",
    "Email: aris.adams@kellogg.com\n",
    "From: choc4sale@aol.com <choc4sale@aol.com>\n",
    "Sent: Tuesday, February 7, 2023 11:57 AM\n",
    "To: Adams, Animequom <Aris.Adams@kellogg.com>\n",
    "Cc: marie-pierre_bolduc@barry-callebaut.com <marie-pierre_bolduc@barry-callebaut.com>; matthew_steinmetz@barry-callebaut.com <matthew_steinmetz@barry-callebaut.com>; stacey_wing@barry-callebaut.com <stacey_wing@barry-callebaut.com>\n",
    "Subject: [EXTERNAL] Quick Question regarding Project Chainsaw and the Original Chocolate Chip PopTart\n",
    " \n",
    "This Message Is From an External Sender\n",
    "This message came from outside your organization.\n",
    " \n",
    "Hi Aris !\n",
    "Nothing urgent. I've been in conversation with Marie-Pierre Bolduc regarding the 12M Pareve Chocolate Chip for the Chocolate Chip Pancake PopTart (Chainsaw) and we keep thinking about Kellogg bringing back the Original Chocolate Chip PopTart using a 12M Chip as a Topping instead of the current 15M Pareve chip that the item was launched with... Is it a certainty that Kellogg will bring back the CC PopTart soon using the 12M Pareve Chip ?\n",
    "\n",
    "If this is the case, Marie and I are thinking with Chainsaw as well as the re-start of the Original Chocolate Chip Poptart , the most logical solution is to create a new SKU of the 12M Pareve chip into a 50 lb. or 30 lb. carton and use it for both projects. Let us know your thoughts and if indeed we are like minded regarding this new SKU solution, kindly indicate Kellogg's preference for 50s or 30s on the carton's net weight. Thank you in advance.  John\n",
    "\n",
    "\n",
    "--- END OF DOCUMENT ---\n",
    "\n",
    "Now extract and categorize the relevant structured attributes and their values into 8 categories, each with explicit and inferred sections, as per the guidelines. Provide only valid JSON output without commentary or markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "        result = openai_call(\n",
    "            sys_prompt=system_prompt,\n",
    "            prompt_struc=user_prompt,\n",
    "            additional_message=\"extract_attributes_from_rd_brief\"\n",
    "        )\n",
    "\n",
    "        if result:\n",
    "            with open(\"e_a_00147036.json\", \"w\") as f:\n",
    "                json.dump(json.loads(result), f, indent=2)\n",
    "                print(\"\\n‚úÖ Extracted attributes saved to 'e_a_00147036.json'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Excel saved to: attribute_output_00147036.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"allergen_items\", \"claims_certifications\", \"ingredients_composition\",\n",
    "    \"legal_specifications\", \"nutritional_values\", \"packaging_information\",\n",
    "    \"sales_commercial\", \"technical_specifications\"\n",
    "]\n",
    "\n",
    "def format_as_json_string(data: dict) -> str:\n",
    "    if not data:\n",
    "        return \"{}\"\n",
    "    return json.dumps(data, indent=2, ensure_ascii=False)\n",
    "\n",
    "def json_to_structured_excel(json_file: str, output_excel: str):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for idx, category in enumerate(CATEGORIES, start=1):\n",
    "        explicit_attrs = data.get(category, {}).get(\"explicit\", {})\n",
    "        inferred_attrs = data.get(category, {}).get(\"inferred\", {})\n",
    "\n",
    "        row = {\n",
    "            \"S. No.\": idx,\n",
    "            \"Category_Name\": category,\n",
    "            \"Explicit Attributes\": format_as_json_string(explicit_attrs),\n",
    "            \"Inferred Attributes\": format_as_json_string(inferred_attrs)\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_excel(output_excel, index=False)\n",
    "\n",
    "    # Adjust formatting using openpyxl\n",
    "    wb = load_workbook(output_excel)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Set column widths (40 characters wide) and wrap text\n",
    "    for col in range(1, ws.max_column + 1):\n",
    "        col_letter = get_column_letter(col)\n",
    "        ws.column_dimensions[col_letter].width = 40\n",
    "        for row in range(2, ws.max_row + 1):  # Skip header\n",
    "            cell = ws.cell(row=row, column=col)\n",
    "            cell.alignment = Alignment(wrap_text=True, vertical=\"top\")\n",
    "\n",
    "    # Set row height to 409 for all rows\n",
    "    for row in range(2, ws.max_row + 1):  # Skip header row\n",
    "        ws.row_dimensions[row].height = 409\n",
    "\n",
    "    wb.save(output_excel)\n",
    "    print(f\"‚úÖ Excel saved to: {output_excel}\")\n",
    "\n",
    "# Example usage\n",
    "json_to_structured_excel(\"e_a_00147036.json\", \"attribute_output_00147036.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Excel file saved to: categorized_attributes_00170112.xlsx\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the 8 attribute categories\n",
    "# CATEGORIES = [\n",
    "#     \"allergen_items\", \"claims_certifications\", \"ingredients_composition\",\n",
    "#     \"legal_specifications\", \"nutritional_values\", \"packaging_information\",\n",
    "#     \"sales_commercial\", \"technical_specifications\"\n",
    "# ]\n",
    "\n",
    "# def format_value(val):\n",
    "#     if isinstance(val, dict):\n",
    "#         return \"; \".join(f\"{k}: {format_value(v)}\" for k, v in val.items())\n",
    "#     elif isinstance(val, list):\n",
    "#         return \", \".join(str(v) for v in val)\n",
    "#     return str(val)\n",
    "\n",
    "# def convert_structured_attributes(json_file: str, output_excel: str):\n",
    "#     with open(json_file, 'r') as f:\n",
    "#         attributes_data = json.load(f)\n",
    "\n",
    "#     # Prepare a structure to hold all values\n",
    "#     attribute_lookup = {cat: {\"explicit\": {}, \"inferred\": {}} for cat in CATEGORIES}\n",
    "#     all_attribute_names = set()\n",
    "\n",
    "#     # Extract values and track attribute names\n",
    "#     for category in CATEGORIES:\n",
    "#         if category in attributes_data:\n",
    "#             for source_type in [\"explicit\", \"inferred\"]:\n",
    "#                 items = attributes_data[category].get(source_type, {})\n",
    "#                 for attr_name, attr_obj in items.items():\n",
    "#                     value = format_value(attr_obj.get(\"value\", \"\"))\n",
    "#                     attribute_lookup[category][source_type][attr_name] = value\n",
    "#                     all_attribute_names.add(attr_name)\n",
    "\n",
    "#     # Prepare rows\n",
    "#     rows = []\n",
    "#     for attr in sorted(all_attribute_names):\n",
    "#         row = {\"Attribute Name\": attr}\n",
    "#         for category in CATEGORIES:\n",
    "#             row[f\"{category} (explicit)\"] = attribute_lookup[category][\"explicit\"].get(attr, \"\")\n",
    "#             row[f\"{category} (inferred)\"] = attribute_lookup[category][\"inferred\"].get(attr, \"\")\n",
    "#         rows.append(row)\n",
    "\n",
    "#     # Build DataFrame\n",
    "#     df = pd.DataFrame(rows)\n",
    "\n",
    "#     # Ensure all expected columns exist\n",
    "#     all_columns = [\"Attribute Name\"] + [f\"{cat} (explicit)\" for cat in CATEGORIES] + [f\"{cat} (inferred)\" for cat in CATEGORIES]\n",
    "#     for col in all_columns:\n",
    "#         if col not in df.columns:\n",
    "#             df[col] = \"\"\n",
    "\n",
    "#     # Reorder columns\n",
    "#     df = df[all_columns]\n",
    "\n",
    "#     # Export to Excel\n",
    "#     df.to_excel(output_excel, index=False)\n",
    "#     print(f\"‚úÖ Excel file saved to: {output_excel}\")\n",
    "\n",
    "# # Example usage\n",
    "# convert_structured_attributes(\"extracted_attributes00170112.json\", \"categorized_attributes_00170112.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"01_25th_June_product_catalogue.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28680, 236)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Material Code', 'Legislation', 'Cluster', 'Cluster_Label', 'Legal_Denomination', 'Legislation_Description', 'Min_Dry_Cocoa_Solids', 'Dry_Milk_Solids', 'MilkFat', 'Dry_Fat_Free_Cocoa_Solids', 'Typical_Chocolate_Liquor', 'Typical_Cocoa_Content', 'Total_Legal_Fat_Content', 'MilkFat_Chocolate_Part', 'Dry_Milk_Solids_On_Total_Production', 'Dry_Milk_Solids_Chocolate_Part', 'Typical_Nonfat_Milk_Solids', 'Typ_Nonfat_Cocoa_Sol_Choc_Part', 'Sum_Dry_Cocoa_And_Milk_Solids', 'Cocoa_Butter_Content', 'Alkalizing_Agent_K2CO3_DFFCS', 'Component', 'Item', 'Level', 'Material_Group', 'Material_Type', 'Parent_Material', 'Parent_Material_Label', 'Source_Generated_Field', 'components_Specifications', 'Sugars_g', 'Salt_g', 'Trans_Fatty_Acid_TFa_g', 'Energy_Value_Kcal', 'Energy_Value_Kj', 'Protein_g', 'Protein_DV_perc', 'Total_Carbohydrates_g', 'Total_Carbohydrates_DV_perc', 'Saturated_Fatty_Acid_g', 'Calories_From_Fat', 'Cholesterol_mg', 'Cholesterol_DV_perc', 'Total_Fat_DV_perc', 'Fibre_g', 'Dietary_Fibre_DV_perc', 'Vitamin_A_mcg', 'Vitamin_C_mg', 'Sodium_mg', 'Sodium_DV_perc', 'Iron_mg', 'Calcium_mg', 'Available_Carbohydrates_g', 'Total_Fat_g', 'Allergen_Statements', 'Contains_Milk_Proteins', 'Contains_Egg_Products', 'Contains_Soy_Proteins', 'Contains_Wheat', 'Contains_Rye', 'Contains_Fish', 'Contains_Crustacean_And_Shell_Fish', 'Contains_Hazelnuts_Almonds', 'Contains_Peanuts', 'Contains_Sulphite_E220_E227', 'Contains_Celery', 'Contains_Sesame_Products', 'Suitable_For_Vegetarians', 'Suitable_For_Vegans', 'Hazelnut_Oil_Almond_Oil', 'Contains_Sesame_Oil', 'Contains_Peanut_Oil', 'Contains_Mustard', 'Contains_Molluscs', 'Contains_Lupin', 'Contains_Buckwheat', 'Plant_BoM_Owner_Short', 'Underlying_Liquid', 'Dimensions_Vibration_Drops_EU_Short', 'Project_Number_Short', 'Dimensions_Production_Tools_Us', 'Material_Description', 'Base_Type', 'Moulding_Type', 'Product_Type', 'Colour_TF', 'Project_Manager', 'Dimensions_Vibration_Drops_EU', 'Project_Phase', 'Certification', 'Base_Colour', 'Additional_Colour', 'Kosher_Certificate', 'Country_Claim', 'Plant_BoM_Owner', 'Type_3_Short', 'Dosage_Per_200ml_Cold_Milk', 'Dosage_Per_200ml_Cold_Water', 'Dosage_Per_200ml_Hot_Milk', 'Dosage_Per_200Ml_Hot_Water', 'Fineness_Type', 'Colour_L_Value_From', 'pH_From', 'pH_To', 'Normalised_Linear_Mpa_S_From', 'Normalised_Linear_Viscosity_mPaS_To', 'Normalised_Casson_Mpa_S_From', 'Normalised_Casson_Mpa_S_To', 'Normalised_Yield_Pa_From', 'Normalised_Yield_Pa_To', 'Fineness_Micrometer_From', 'Fineness_Micrometer_To', 'Dimensions_Length_From', 'Dimensions_Length_To', 'Dimensions_Width_From', 'Dimensions_Width_To', 'Dimensions_Height_From', 'Dimensions_Count_Kg_From', 'Dimensions_Count_Kg_To', 'Dimensions_Sieve_Fraction_From', 'Dimensions_Sieve_Fraction_To', 'Protein', 'Dimensions_Count_lb_From', 'Dimensions_Count_lb_To', 'Brookfield_40C_S27_20_RPM_From', 'Brookfield_40C_S27_20_RPM_To', 'Brookfield_50C_S27_20_RPM_From', 'Brookfield_50C_S27_20_RPM_To', 'Brookfield_50C_S27_Regression_From', 'Brookfield_50C_S27_Regression_To', 'Brookfield_50C_S27_Yield_From', 'Brookfield_50C_S27_Yield_To', 'Brookfield_40C_S27_Regression_From', 'Brookfield_40C_S27_Regression_To', 'Brookfield_40C_S27_Yield_From', 'Brookfield_40C_S27_Yield_To', 'Water_Activity_From', 'Water_Activity_To', 'Shelflife', 'Bulk_Density_Untapped_From', 'Bulk_Density_Untapped_To', 'Bulk_Density_Tapped_x100_From', 'Bulk_Density_Tapped_x100_To', 'Dosage_Test_Grams_From', 'Dosage_Test_Grams_To', 'Material_Group_Short', 'Packaging_Info', 'Sales_Organisation', 'Plant_Where_Produced_OR_Available', 'Primary_Weight_Unit', 'Primary_Count_Unit', 'Material_Group_Long', 'Brand', 'Kosher_recipe_not_certificate_', 'Marking', 'Primary_weight', 'Primary_Count', 'NGM_Status', 'Regional_Supply_Policy_West_Europe', 'Additional_Premium_Group', 'Regional_Speciality_Category_West_Europe', 'Regional_Speciality_Category_East_Europe', 'Regional_Speciality_Category_US', 'Regional_Speciality_Category_Asia', 'Regional_Supply_Policy_US', 'Mass_Balance_Certification', 'Western_EU_BC_Selection', 'Regional_Sales_Forecast_West_Europe', 'Regional_Sales_Forecast_East_Europe', 'Regional_Sales_Forecast_US', 'Regional_Sales_Forecast_Asia', 'Regional_Premium_Category_West_Europe', 'Regional_Premium_Category_East_Europe', 'Regional_Premium_Category_US', 'Regional_Premium_Category_Asia', 'Product_Category_West_Europe_', 'Product_Category_East_Europe', 'Product_Category_US', 'Product_Category_Asia', 'Eastern_EU_BC_Selection', 'Standard_Range_Mexico', 'Standard_Range_US', 'Commercial_Name', 'Commodity_Code', 'Lifecycle_status', 'Distribution_Channel', 'Calculated_Price_Currency', 'Sales_Organisation_Distribution_Channel', 'Delivery_Unit_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_In_UoM', 'Delivery_unit_Qty_In_UoM', 'Replenishment_Lead_Time', 'Sales_Last_12_Months_North_America_in_KG', 'Sales_Last_12_Months_Asia_in_KG_', 'Sales_Last_12_Months_EEMEA_in_KG_', 'Sales_Last_12_Months_West_Europe_in_KG', 'Sales_Last_12_Months_South_America_in_KG', 'Calculated_Price', 'Sales_Last_12_Months_Total_In_Kg', 'Contains_Hydrogenated', 'Fat', 'Polyols', 'Nuts_and_Almonds', 'Total_Fat_On_Spec_perc_From', 'Total_Fat_On_Spec_perc_To', 'Hydrogenated', 'Core_OR_Extended', 'Core_Region', 'Core_Country', 'Core_Segment', 'Core_Subsegment', 'Category', 'Region', 'Customer_Dedication', 'Proj_Phase', 'Pack_Code', 'Base_Unit_Of_Measure', 'Smallest_Unit_Weight_In_Kg', 'Sample_Unit', 'Units_Per_Layer', 'Units_Per_Pallet', 'Pallet', 'Pallet_Net_Weight_In_Kg', 'Pallet_Gross_Weight_In_Kg', 'Length', 'Width', 'Height', 'Pallet_Type', 'Sales_Unit', 'Layer', 'Delivery_Unit_AUM', 'Certification_Tag', 'Colour_Type_Tag', 'Flavor_Type_Tag', 'Ingredients_Tag']\n"
     ]
    }
   ],
   "source": [
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "print(len(list(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

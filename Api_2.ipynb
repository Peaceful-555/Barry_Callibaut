{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Calling OpenAI API with deployment: gpt-4o, Retry: 1\n",
      "INFO:__main__:OPENAI CALL MESSAGE: Retry 1 | Func: extract_attributes_from_rd_brief | Duration: 19.679s completion_tokens: 1679 | prompt_tokens: 5205 | total_tokens: 6884 | completion_tokens_details: {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0} | prompt_tokens_details: {'audio_tokens': 0, 'cached_tokens': 0} | \n",
      "INFO:__main__:Token Usage Summary: {'extract_attributes_from_rd_brief': 6884}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 OpenAI Response:\n",
      " {\n",
      "  \"allergen_items\": {\n",
      "    \"explicit\": {\n",
      "      \"Contains_Soy_Proteins\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Contains_Milk_Proteins\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {\n",
      "      \"Contains_Palm_Oil\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Palm oil is explicitly listed in the ingredients.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"claims_certifications\": {\n",
      "    \"explicit\": {\n",
      "      \"Non_GMO\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"No_Hydrogenated_Fats\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Ionizing_Radiation_Free\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {}\n",
      "  },\n",
      "  \"ingredients_composition\": {\n",
      "    \"explicit\": {\n",
      "      \"Sugar\": {\n",
      "        \"value\": \"Present\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Vegetable_Fats\": {\n",
      "        \"value\": \"Palm, Sunflower, Soybean, Rapeseed\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Dry_Milk_Solids\": {\n",
      "        \"value\": \"Partially defatted milk powder\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Emulsifier\": {\n",
      "        \"value\": \"Soy Lecithin (E322)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Natural_Flavoring\": {\n",
      "        \"value\": \"Vanilla\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Cocoa_Powder\": {\n",
      "        \"value\": \"16% (for dark chocolate substitute)\",\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {\n",
      "      \"Contains_Vegetable_Fats\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Vegetable fats are explicitly listed in the ingredients.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"legal_specifications\": {\n",
      "    \"explicit\": {\n",
      "      \"Legislation\": {\n",
      "        \"value\": \"2003/1829/EC and 2003/1830/EC\",\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {}\n",
      "  },\n",
      "  \"nutritional_values\": {\n",
      "    \"explicit\": {\n",
      "      \"Energy_Value_Kcal\": {\n",
      "        \"value\": \"585 (Caravella Cover White), 576 (Centramerica Bianco Dischi), 547 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Energy_Value_Kj\": {\n",
      "        \"value\": \"2438 (Caravella Cover White), 2403 (Centramerica Bianco Dischi), 2279 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Total_Fat_g\": {\n",
      "        \"value\": \"39.0 (Caravella Cover White), 36.0 (Centramerica Bianco Dischi), 34.0 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Saturated_Fatty_Acid_g\": {\n",
      "        \"value\": \"17.0 (Caravella Cover White), 31.0 (Centramerica Bianco Dischi), 33.0 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Trans_Fatty_Acid_TFa_g\": {\n",
      "        \"value\": \"< 0.5 (Caravella Cover White), 0.0 (Centramerica Bianco Dischi), 0.2 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Total_Carbohydrates_g\": {\n",
      "        \"value\": \"50.0 (Caravella Cover White), 59.0 (Centramerica Bianco Dischi), 54.0 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Sugars_g\": {\n",
      "        \"value\": \"50.0 (Caravella Cover White), 59.0 (Centramerica Bianco Dischi), 52.0 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Protein_g\": {\n",
      "        \"value\": \"8.5 (Caravella Cover White), 4.0 (Centramerica Bianco Dischi), 3.7 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Salt_g\": {\n",
      "        \"value\": \"0.36 (Caravella Cover White), 0.17 (Centramerica Bianco Dischi), 0.01 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Sodium_mg\": {\n",
      "        \"value\": \"143 (Caravella Cover White), 67 (Centramerica Bianco Dischi), 5 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {}\n",
      "  },\n",
      "  \"packaging_information\": {\n",
      "    \"explicit\": {\n",
      "      \"Primary_Weight_Unit\": {\n",
      "        \"value\": \"5 kg (Caravella Cover White), 10 kg (Centramerica Bianco Dischi), 20 kg (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Packaging_Type\": {\n",
      "        \"value\": \"Polypropylene buckets (Caravella Cover White), Cardboard boxes with polyethylene lining (Centramerica Bianco Dischi and Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Pallet_Net_Weight_In_Kg\": {\n",
      "        \"value\": \"600 kg (Caravella Cover White), 810 kg (Centramerica Bianco Dischi), 900 kg (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {}\n",
      "  },\n",
      "  \"sales_commercial\": {\n",
      "    \"explicit\": {\n",
      "      \"Shelflife\": {\n",
      "        \"value\": \"360 days (Caravella Cover White and Centramerica Bianco Dischi), 540 days (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {}\n",
      "  },\n",
      "  \"technical_specifications\": {\n",
      "    \"explicit\": {\n",
      "      \"Moisture_Content\": {\n",
      "        \"value\": \"≤ 1.0%\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Fat_Content\": {\n",
      "        \"value\": \"38.0 - 40.0% (Caravella Cover White), 35.0 - 37.0% (Centramerica Bianco Dischi), 33 - 35% (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Viscosity_mPa_s\": {\n",
      "        \"value\": \"750-1100 (Centramerica Bianco Dischi), 1700-2000 (Caribe Fondente Dischi)\",\n",
      "        \"source\": \"explicit\"\n",
      "      },\n",
      "      \"Microbiological_Characteristics\": {\n",
      "        \"value\": {\n",
      "          \"Total_Bacteria\": \"≤ 5,000 CFU/g\",\n",
      "          \"Yeast\": \"≤ 50 CFU/g\",\n",
      "          \"Mold\": \"≤ 50 CFU/g\",\n",
      "          \"Enterobacteria\": \"≤ 10 CFU/g\",\n",
      "          \"Salmonella\": \"Absent in 100 g\",\n",
      "          \"E_Coli\": \"≤ 10 CFU/g\"\n",
      "        },\n",
      "        \"source\": \"explicit\"\n",
      "      }\n",
      "    },\n",
      "    \"inferred\": {\n",
      "      \"Freezer_Stability\": {\n",
      "        \"value\": \"Yes\",\n",
      "        \"source\": \"inferred\",\n",
      "        \"note\": \"Explicit mention of freezing capability in product description.\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "✅ Extracted attributes saved to 'e_a2407_attchement00145912.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ------------------ Load environment variables ------------------\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------ Logging setup ------------------\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# ------------------ Azure Credentials ------------------\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=os.getenv(\"AZURE_TENANT_ID\"),\n",
    "    client_id=os.getenv(\"AZURE_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"AZURE_CLIENT_SECRET\"),\n",
    ")\n",
    "\n",
    "# ------------------ Globals ------------------\n",
    "TOKEN_USAGE = {}\n",
    "TOTAL_API_CALL = 0\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# ------------------ Azure Document Intelligence Client ------------------\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_KEY = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "if not AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT or not AZURE_DOCUMENT_INTELLIGENCE_KEY:\n",
    "    raise EnvironmentError(\"Missing Azure Document Intelligence credentials\")\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT,\n",
    "    credential=AzureKeyCredential(AZURE_DOCUMENT_INTELLIGENCE_KEY)\n",
    ")\n",
    "\n",
    "# ------------------ OCR Function ------------------\n",
    "def process_file_new_ocr(file_path: str) -> List[Dict]:\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    logging.info(f\"Processing file: {file_path.name}\")\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        document_bytes = f.read()\n",
    "\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        model_id=\"prebuilt-read\",\n",
    "        body=document_bytes,\n",
    "    )\n",
    "    result = poller.result()\n",
    "\n",
    "    attachment_list = []\n",
    "    for page in result.pages:\n",
    "        text = \" \".join([line.content for line in page.lines])\n",
    "        attachment_list.append({\n",
    "            \"title\": file_path.name,\n",
    "            \"pagenum\": page.page_number,\n",
    "            \"content\": text\n",
    "        })\n",
    "\n",
    "    return attachment_list\n",
    "\n",
    "# ------------------ Azure OpenAI LLM Clients ------------------\n",
    "AzureChatOpenAI.model_rebuild()\n",
    "\n",
    "def llm():\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    return AzureChatOpenAI(\n",
    "        azure_deployment=AZURE_OPENAI_DEPLOYMENT,\n",
    "        api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        openai_api_key=access_token,\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    )\n",
    "\n",
    "def openai_llm():\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    return AzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "        api_key=access_token,\n",
    "    )\n",
    "\n",
    "def embeddings():\n",
    "    access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    return AzureOpenAIEmbeddings(\n",
    "        model=os.getenv(\"EMBEDDING_AZURE_OPENAI_DEPLOYMENT\"),\n",
    "        azure_endpoint=os.getenv(\"EMBEDDING_AZURE_OPENAI_ENDPOINT\"),\n",
    "        openai_api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "        api_key=access_token,\n",
    "    )\n",
    "\n",
    "# ------------------ OpenAI Call Utilities ------------------\n",
    "def add_token_usage_logs(llm_output, message=\"\"):\n",
    "    token_usage_string = \"\"\n",
    "    for key, value in llm_output.to_dict().get(\"usage\", {}).items():\n",
    "        token_usage_string += f\"{key}: {value} | \"\n",
    "    logger.info(f\"{message} {token_usage_string}\")\n",
    "    return llm_output.to_dict().get(\"usage\", {}).get(\"total_tokens\", 0)\n",
    "\n",
    "def openai_call(sys_prompt, prompt_struc, deployment_name=AZURE_OPENAI_DEPLOYMENT, additional_message=\"\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt_struc},\n",
    "    ]\n",
    "    return _call_openai(messages, deployment_name, additional_message)\n",
    "\n",
    "def _call_openai(messages, deployment_name, additional_message):\n",
    "    global TOTAL_API_CALL\n",
    "    max_retries = 5\n",
    "    for current_retry in range(max_retries):\n",
    "        try:\n",
    "            time.sleep(5)\n",
    "            logger.info(f\"Calling OpenAI API with deployment: {deployment_name}, Retry: {current_retry + 1}\")\n",
    "            start_time = time.time()\n",
    "            response = openai_llm().chat.completions.create(\n",
    "                model=deployment_name,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            TOTAL_API_CALL += 1\n",
    "            duration = round(time.time() - start_time, 3)\n",
    "            output = response.choices[0].message.content\n",
    "            if output:\n",
    "                msg = f\"OPENAI CALL MESSAGE: Retry {current_retry + 1} | Func: {additional_message} | Duration: {duration}s\"\n",
    "                total_tokens = add_token_usage_logs(response, message=msg)\n",
    "                TOKEN_USAGE[additional_message] = TOKEN_USAGE.get(additional_message, 0) + total_tokens\n",
    "                logger.info(f\"Token Usage Summary: {TOKEN_USAGE}\")\n",
    "                print(\"\\n🔹 OpenAI Response:\\n\", output)\n",
    "                return output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OpenAI Call Error: {e}\")\n",
    "    logger.warning(\"Maximum retries reached. No response returned.\")\n",
    "    return None\n",
    "\n",
    "# ------------------ MAIN EXECUTION BLOCK ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    test_file = \"OPP-00145912 - Глазурь темная спецификация ММ.pdf\"\n",
    "\n",
    "    try:\n",
    "        ocr_pages = process_file_new_ocr(test_file)\n",
    "        extracted_text = \"\\n\".join([p[\"content\"] for p in ocr_pages])\n",
    "\n",
    "        system_prompt = \"\"\"\n",
    "You are a domain-aware assistant that extracts structured product development attributes from customer R&D or supplier brief documents.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Your Role:\n",
    "You are interpreting a **customer brief** — a document that includes both direct requirements and contextual information. Your job is to:\n",
    "1. Extract **explicit** attributes: those that are **directly requested by the customer** in the brief.\n",
    "2. Extract **inferred** attributes: high-confidence values **logically implied** by the brief’s context, product type, or regulatory standards, with a supporting `\"note\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Context:\n",
    "You are Barry Callebaut. Your customers are manufacturers and retailers of consumer-facing chocolate products.  \n",
    "You have received a document that is a **customer brief** meant for product development.  \n",
    "This document may contain a mixture of customer inputs, internal notes, descriptions, email threads, or conversational noise.  \n",
    "Your job is to **discard irrelevant information** and focus **only on the actual customer requirements**, which will be used to help **R&D suggest matching SKUs from Barry Callebaut’s portfolio**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Mandatory Instructions:\n",
    "- Maximize the number of attributes extracted from the brief.\n",
    "- Any **exclusionary constraint** stated by the customer (e.g., “without milk”, “no added sugar”, “must not contain nuts”) must be treated as **explicit**, not inferred.These are direct requirements and must be captured under `\"explicit\"` in the output — even if phrased negatively.\n",
    "- Examples of explicit negative requirements:\n",
    "  - “without cinnamon”\n",
    "  - “no palm oil”\n",
    "  - “no dairy”\n",
    "  - “does not contain hazelnuts”\n",
    "-Include customer preferences such as **preferred but not mandatory**,**ideally** or **best if** under **explicit** attributes, since they are clearly stated in the brief. These are still considered explicit even if flexible or not required.\n",
    "-Treat direct product form requests such as “buttons,” “chunks,” “chips,” “drops,” “sheets,” etc., as explicit if clearly mentioned by the customer — even if no SKU is chosen or pack size is given.Do not classify these as inferred, unless the form is deduced from context (e.g., “for decorating” or “for enrobing”).\n",
    "- **Always extract** the following fields if present — even if not explicitly requested:\n",
    "\n",
    "['Material Code', 'Legislation', 'Cluster', 'Cluster_Label', 'Legal_Denomination', 'Legislation_Description', 'Min_Dry_Cocoa_Solids', 'Dry_Milk_Solids', 'MilkFat', 'Dry_Fat_Free_Cocoa_Solids', 'Typical_Chocolate_Liquor', 'Typical_Cocoa_Content', 'Total_Legal_Fat_Content', 'MilkFat_Chocolate_Part', 'Dry_Milk_Solids_On_Total_Production', 'Dry_Milk_Solids_Chocolate_Part', 'Typical_Nonfat_Milk_Solids', 'Typ_Nonfat_Cocoa_Sol_Choc_Part', 'Sum_Dry_Cocoa_And_Milk_Solids', 'Cocoa_Butter_Content', 'Alkalizing_Agent_K2CO3_DFFCS', 'Component', 'Item', 'Level', 'Material_Group', 'Material_Type', 'Parent_Material', 'Parent_Material_Label', 'Source_Generated_Field', 'components_Specifications', 'Sugars_g', 'Salt_g', 'Trans_Fatty_Acid_TFa_g', 'Energy_Value_Kcal', 'Energy_Value_Kj', 'Protein_g', 'Protein_DV_perc', 'Total_Carbohydrates_g', 'Total_Carbohydrates_DV_perc', 'Saturated_Fatty_Acid_g', 'Calories_From_Fat', 'Cholesterol_mg', 'Cholesterol_DV_perc', 'Total_Fat_DV_perc', 'Fibre_g', 'Dietary_Fibre_DV_perc', 'Vitamin_A_mcg', 'Vitamin_C_mg', 'Sodium_mg', 'Sodium_DV_perc', 'Iron_mg', 'Calcium_mg', 'Available_Carbohydrates_g', 'Total_Fat_g', 'Allergen_Statements', 'Contains_Milk_Proteins', 'Contains_Egg_Products', 'Contains_Soy_Proteins', 'Contains_Wheat', 'Contains_Rye', 'Contains_Fish', 'Contains_Crustacean_And_Shell_Fish', 'Contains_Hazelnuts_Almonds', 'Contains_Peanuts', 'Contains_Sulphite_E220_E227', 'Contains_Celery', 'Contains_Sesame_Products', 'Suitable_For_Vegetarians', 'Suitable_For_Vegans', 'Hazelnut_Oil_Almond_Oil', 'Contains_Sesame_Oil', 'Contains_Peanut_Oil', 'Contains_Mustard', 'Contains_Molluscs', 'Contains_Lupin', 'Contains_Buckwheat', 'Plant_BoM_Owner_Short', 'Underlying_Liquid', 'Dimensions_Vibration_Drops_EU_Short', 'Project_Number_Short', 'Dimensions_Production_Tools_Us', 'Material_Description', 'Base_Type', 'Moulding_Type', 'Product_Type', 'Colour_TF', 'Project_Manager', 'Dimensions_Vibration_Drops_EU', 'Project_Phase', 'Certification', 'Base_Colour', 'Additional_Colour', 'Kosher_Certificate', 'Country_Claim', 'Plant_BoM_Owner', 'Type_3_Short', 'Dosage_Per_200ml_Cold_Milk', 'Dosage_Per_200ml_Cold_Water', 'Dosage_Per_200ml_Hot_Milk', 'Dosage_Per_200Ml_Hot_Water', 'Fineness_Type', 'Colour_L_Value_From', 'pH_From', 'pH_To', 'Normalised_Linear_Mpa_S_From', 'Normalised_Linear_Viscosity_mPaS_To', 'Normalised_Casson_Mpa_S_From', 'Normalised_Casson_Mpa_S_To', 'Normalised_Yield_Pa_From', 'Normalised_Yield_Pa_To', 'Fineness_Micrometer_From', 'Fineness_Micrometer_To', 'Dimensions_Length_From', 'Dimensions_Length_To', 'Dimensions_Width_From', 'Dimensions_Width_To', 'Dimensions_Height_From', 'Dimensions_Count_Kg_From', 'Dimensions_Count_Kg_To', 'Dimensions_Sieve_Fraction_From', 'Dimensions_Sieve_Fraction_To', 'Protein', 'Dimensions_Count_lb_From', 'Dimensions_Count_lb_To', 'Brookfield_40C_S27_20_RPM_From', 'Brookfield_40C_S27_20_RPM_To', 'Brookfield_50C_S27_20_RPM_From', 'Brookfield_50C_S27_20_RPM_To', 'Brookfield_50C_S27_Regression_From', 'Brookfield_50C_S27_Regression_To', 'Brookfield_50C_S27_Yield_From', 'Brookfield_50C_S27_Yield_To', 'Brookfield_40C_S27_Regression_From', 'Brookfield_40C_S27_Regression_To', 'Brookfield_40C_S27_Yield_From', 'Brookfield_40C_S27_Yield_To', 'Water_Activity_From', 'Water_Activity_To', 'Shelflife', 'Bulk_Density_Untapped_From', 'Bulk_Density_Untapped_To', 'Bulk_Density_Tapped_x100_From', 'Bulk_Density_Tapped_x100_To', 'Dosage_Test_Grams_From', 'Dosage_Test_Grams_To', 'Material_Group_Short', 'Packaging_Info', 'Sales_Organisation', 'Plant_Where_Produced_OR_Available', 'Primary_Weight_Unit', 'Primary_Count_Unit', 'Material_Group_Long', 'Brand', 'Kosher_recipe_not_certificate_', 'Marking', 'Primary_weight', 'Primary_Count', 'NGM_Status', 'Regional_Supply_Policy_West_Europe', 'Additional_Premium_Group', 'Regional_Speciality_Category_West_Europe', 'Regional_Speciality_Category_East_Europe', 'Regional_Speciality_Category_US', 'Regional_Speciality_Category_Asia', 'Regional_Supply_Policy_US', 'Mass_Balance_Certification', 'Western_EU_BC_Selection', 'Regional_Sales_Forecast_West_Europe', 'Regional_Sales_Forecast_East_Europe', 'Regional_Sales_Forecast_US', 'Regional_Sales_Forecast_Asia', 'Regional_Premium_Category_West_Europe', 'Regional_Premium_Category_East_Europe', 'Regional_Premium_Category_US', 'Regional_Premium_Category_Asia', 'Product_Category_West_Europe_', 'Product_Category_East_Europe', 'Product_Category_US', 'Product_Category_Asia', 'Eastern_EU_BC_Selection', 'Standard_Range_Mexico', 'Standard_Range_US', 'Commercial_Name', 'Commodity_Code', 'Lifecycle_status', 'Distribution_Channel', 'Calculated_Price_Currency', 'Sales_Organisation_Distribution_Channel', 'Delivery_Unit_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_In_UoM', 'Delivery_unit_Qty_In_UoM', 'Replenishment_Lead_Time', 'Sales_Last_12_Months_North_America_in_KG', 'Sales_Last_12_Months_Asia_in_KG_', 'Sales_Last_12_Months_EEMEA_in_KG_', 'Sales_Last_12_Months_West_Europe_in_KG', 'Sales_Last_12_Months_South_America_in_KG', 'Calculated_Price', 'Sales_Last_12_Months_Total_In_Kg', 'Contains_Hydrogenated', 'Fat', 'Polyols', 'Nuts_and_Almonds', 'Total_Fat_On_Spec_perc_From', 'Total_Fat_On_Spec_perc_To', 'Hydrogenated', 'Core_OR_Extended', 'Core_Region', 'Core_Country', 'Core_Segment', 'Core_Subsegment', 'Category', 'Region', 'Customer_Dedication', 'Proj_Phase', 'Pack_Code', 'Base_Unit_Of_Measure', 'Smallest_Unit_Weight_In_Kg', 'Sample_Unit', 'Units_Per_Layer', 'Units_Per_Pallet', 'Pallet', 'Pallet_Net_Weight_In_Kg', 'Pallet_Gross_Weight_In_Kg', 'Length', 'Width', 'Height', 'Pallet_Type', 'Sales_Unit', 'Layer', 'Delivery_Unit_AUM', 'Certification_Tag', 'Colour_Type_Tag', 'Flavor_Type_Tag', 'Ingredients_Tag']\n",
    "\n",
    "- Regardless of how they appear in the brief, **always extract** the following attributes:\n",
    "  - `Product_Type`\n",
    "  - `Base_Type`\n",
    "  - `Moulding_Type`\n",
    "  - `Components_Specifications` like sugar, milk ingredients, cocoa butter, natural vanilla extract, emulsifier etc\n",
    "  - `Fat content`\n",
    "  - `pH details`\n",
    "\n",
    "-When extracting \"explicit\" packaging_information:\n",
    "\n",
    "Only include pack sizes for SKUs that are:\n",
    "\n",
    "Explicitly ordered (e.g., “order received”, “confirmed”, “selected”)\n",
    "\n",
    "Or **explicitly requested by the customer**\n",
    "\n",
    "Do not include pack sizes from SKUs that are merely:\n",
    "\n",
    "Sampled\n",
    "\n",
    "Presented as options\n",
    "\n",
    "Rejected or not approved (e.g., “not approved for sale”)\n",
    "\n",
    "To be repacked by third parties\n",
    "\n",
    "Do not aggregate all pack sizes from the document; use only those tied to **final SKUs** the customer has acted on.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂️ Output Categories:\n",
    "Group extracted attributes under the following **8 standard categories**, each containing:\n",
    "- `\"explicit\"`: only those clearly **requested by the customer**\n",
    "- `\"inferred\"`: high-confidence deductions with `\"note\"`\n",
    "\n",
    "1. allergen_items  \n",
    "2. claims_certifications  \n",
    "3. ingredients_composition  \n",
    "4. legal_specifications  \n",
    "5. nutritional_values  \n",
    "6. packaging_information  \n",
    "7. sales_commercial  \n",
    "8. technical_specifications\n",
    "\n",
    "---\n",
    "\n",
    "### Also include **explicit attributes based on product-type logic**:\n",
    "\n",
    "#### Chocolate Type-specific (Dark / Milk / White):\n",
    "- Total fat (% or g/100g)\n",
    "- Minimum dry cocoa solids (%)\n",
    "- Dry fat-free cocoa solids (%)\n",
    "- Milkfat (%) – for milk/white chocolate\n",
    "- Dry milk solids (%)\n",
    "- Fineness type (e.g., FP or micrometer)\n",
    "- Norm linear viscosity (mPa.s)\n",
    "- Casson viscosity (mPa.s)\n",
    "- Yield value (Pa)\n",
    "\n",
    "#### 🍬 Moulding/Shape Specifics:\n",
    "- Length, Width, Height\n",
    "- Vibration (drops)\n",
    "- Primary Count or Count/Unit\n",
    "- Sieve fraction (if relevant)\n",
    "\n",
    "#### 🧪 Compound/Fillings:\n",
    "- Check for “contains hydrogenated” or hydrogenated fats content\n",
    "\n",
    "#### 🥜 If Nuts are Mentioned:\n",
    "- % of nuts or quantity\n",
    "\n",
    "#### 🌍 Export Targets (e.g., EU, US, China):\n",
    "- Legal declaration required\n",
    "- Country-specific regulatory compliance\n",
    "- Typical cocoa content\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Inference Guidelines:\n",
    "\n",
    "Use domain knowledge to **infer high-confidence values**:\n",
    "\n",
    "- PGPR or lecithin implies vegetable fats or emulsifiers  \n",
    "- Codex mention → infer standard ranges for cocoa, milk solids, and sugar  \n",
    "- PGPR ≤ 0.5% → infer presence of vegetable fats  \n",
    "- Enrobing/frozen → infer “Freezer Stability”, “Snap Texture”  \n",
    "- Melting/flow/viscosity mentions → infer “Flowability”  \n",
    "- Export shipping → infer “Shelf Stability”  \n",
    "- RSPO, FSC, Rainforest → infer sustainable sourcing  \n",
    "- “Non-GMO”, “No artificial preservatives/flavors” → infer “free-from” claims  \n",
    "- Codex, EU, FDA law references → infer “Legal Declaration Required”  \n",
    "- “No sugar added” → infer “Low Sugar Claim”  \n",
    "- Box, pouch, bag, sachet → infer “Packaging Format”  \n",
    "- FSC/eco-labels → infer “Sustainable Packaging”  \n",
    "- MOQ, volume, pricing → infer “Sales Channel” or “Indicative Volume”  \n",
    "- “May contain traces…” → infer cross-contamination risk  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Output Rules:\n",
    "- Only include `\"explicit\"` if clearly requested by the customer.\n",
    "- Maximize attribute coverage — both **structured fields** and **inferred logic**\n",
    "- Do not Hallucinate and Do **not fabricate** values without strong support.\n",
    "- Return clean **valid JSON** only — no markdown, comments, or explanations.\n",
    "- For the category **`claims_certifications`**, only include values if **explicitly stated** — do **not infer** any claim or certification.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Output Format:\n",
    "Return structured JSON in this format:\n",
    "\n",
    "{\n",
    "  \"category_name\": {\n",
    "    \"explicit\": {\n",
    "      \"Attribute Name\": {\n",
    "        \"value\": \"...\",\n",
    "        \"source\": \"explicit\"\n",
    "      }\n",
    "    },\n",
    "    \"inferred\": {\n",
    "      \"Attribute Name\": {\n",
    "        \"value\": \"...\",\n",
    "        \"source\": \"inferred\",\n",
    "        \"note\": \"brief justification\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Each `category_name` must match one of the 8 categories above.  \n",
    "The `claims_certifications` category must contain values **only if explicitly stated** by the customer.\n",
    "\"\"\"\n",
    "        user_prompt = f\"\"\"Here is the extracted text from an R&D document:\n",
    "\n",
    "--- START OF DOCUMENT ---\n",
    "{extracted_text}\n",
    "--- END OF DOCUMENT ---\n",
    "\n",
    "Now extract and categorize the relevant structured attributes and their values into 8 categories, each with explicit and inferred sections, as per the guidelines. Provide only valid JSON output without commentary or markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "        result = openai_call(\n",
    "            sys_prompt=system_prompt,\n",
    "            prompt_struc=user_prompt,\n",
    "            additional_message=\"extract_attributes_from_rd_brief\"\n",
    "        )\n",
    "\n",
    "        if result:\n",
    "            with open(\"e_a2407_attachment00145912.json\", \"w\") as f:\n",
    "                json.dump(json.loads(result), f, indent=2)\n",
    "                print(\"\\n✅ Extracted attributes saved to 'e_a2407_attchement00145912.json'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel saved to: attribute_output2307_00177266.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Alignment\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"allergen_items\", \"claims_certifications\", \"ingredients_composition\",\n",
    "    \"legal_specifications\", \"nutritional_values\", \"packaging_information\",\n",
    "    \"sales_commercial\", \"technical_specifications\"\n",
    "]\n",
    "\n",
    "def format_as_json_string(data: dict) -> str:\n",
    "    if not data:\n",
    "        return \"{}\"\n",
    "    return json.dumps(data, indent=2, ensure_ascii=False)\n",
    "\n",
    "def json_to_structured_excel(json_file: str, output_excel: str):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for idx, category in enumerate(CATEGORIES, start=1):\n",
    "        explicit_attrs = data.get(category, {}).get(\"explicit\", {})\n",
    "        inferred_attrs = data.get(category, {}).get(\"inferred\", {})\n",
    "\n",
    "        row = {\n",
    "            \"S. No.\": idx,\n",
    "            \"Category_Name\": category,\n",
    "            \"Explicit Attributes\": format_as_json_string(explicit_attrs),\n",
    "            \"Inferred Attributes\": format_as_json_string(inferred_attrs)\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_excel(output_excel, index=False)\n",
    "\n",
    "    # Adjust formatting using openpyxl\n",
    "    wb = load_workbook(output_excel)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Set column widths (40 characters wide) and wrap text\n",
    "    for col in range(1, ws.max_column + 1):\n",
    "        col_letter = get_column_letter(col)\n",
    "        ws.column_dimensions[col_letter].width = 40\n",
    "        for row in range(2, ws.max_row + 1):  # Skip header\n",
    "            cell = ws.cell(row=row, column=col)\n",
    "            cell.alignment = Alignment(wrap_text=True, vertical=\"top\")\n",
    "\n",
    "    # Set row height to 409 for all rows\n",
    "    for row in range(2, ws.max_row + 1):  # Skip header row\n",
    "        ws.row_dimensions[row].height = 409\n",
    "\n",
    "    wb.save(output_excel)\n",
    "    print(f\"✅ Excel saved to: {output_excel}\")\n",
    "\n",
    "# Example usage\n",
    "json_to_structured_excel(\"e_a2307_00177266.json\", \"attribute_output2307_00177266.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Excel file saved to: categorized_attributes_00170112.xlsx\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the 8 attribute categories\n",
    "# CATEGORIES = [\n",
    "#     \"allergen_items\", \"claims_certifications\", \"ingredients_composition\",\n",
    "#     \"legal_specifications\", \"nutritional_values\", \"packaging_information\",\n",
    "#     \"sales_commercial\", \"technical_specifications\"\n",
    "# ]\n",
    "\n",
    "# def format_value(val):\n",
    "#     if isinstance(val, dict):\n",
    "#         return \"; \".join(f\"{k}: {format_value(v)}\" for k, v in val.items())\n",
    "#     elif isinstance(val, list):\n",
    "#         return \", \".join(str(v) for v in val)\n",
    "#     return str(val)\n",
    "\n",
    "# def convert_structured_attributes(json_file: str, output_excel: str):\n",
    "#     with open(json_file, 'r') as f:\n",
    "#         attributes_data = json.load(f)\n",
    "\n",
    "#     # Prepare a structure to hold all values\n",
    "#     attribute_lookup = {cat: {\"explicit\": {}, \"inferred\": {}} for cat in CATEGORIES}\n",
    "#     all_attribute_names = set()\n",
    "\n",
    "#     # Extract values and track attribute names\n",
    "#     for category in CATEGORIES:\n",
    "#         if category in attributes_data:\n",
    "#             for source_type in [\"explicit\", \"inferred\"]:\n",
    "#                 items = attributes_data[category].get(source_type, {})\n",
    "#                 for attr_name, attr_obj in items.items():\n",
    "#                     value = format_value(attr_obj.get(\"value\", \"\"))\n",
    "#                     attribute_lookup[category][source_type][attr_name] = value\n",
    "#                     all_attribute_names.add(attr_name)\n",
    "\n",
    "#     # Prepare rows\n",
    "#     rows = []\n",
    "#     for attr in sorted(all_attribute_names):\n",
    "#         row = {\"Attribute Name\": attr}\n",
    "#         for category in CATEGORIES:\n",
    "#             row[f\"{category} (explicit)\"] = attribute_lookup[category][\"explicit\"].get(attr, \"\")\n",
    "#             row[f\"{category} (inferred)\"] = attribute_lookup[category][\"inferred\"].get(attr, \"\")\n",
    "#         rows.append(row)\n",
    "\n",
    "#     # Build DataFrame\n",
    "#     df = pd.DataFrame(rows)\n",
    "\n",
    "#     # Ensure all expected columns exist\n",
    "#     all_columns = [\"Attribute Name\"] + [f\"{cat} (explicit)\" for cat in CATEGORIES] + [f\"{cat} (inferred)\" for cat in CATEGORIES]\n",
    "#     for col in all_columns:\n",
    "#         if col not in df.columns:\n",
    "#             df[col] = \"\"\n",
    "\n",
    "#     # Reorder columns\n",
    "#     df = df[all_columns]\n",
    "\n",
    "#     # Export to Excel\n",
    "#     df.to_excel(output_excel, index=False)\n",
    "#     print(f\"✅ Excel file saved to: {output_excel}\")\n",
    "\n",
    "# # Example usage\n",
    "# convert_structured_attributes(\"extracted_attributes00170112.json\", \"categorized_attributes_00170112.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"01_25th_June_product_catalogue.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28680, 236)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Material Code', 'Legislation', 'Cluster', 'Cluster_Label', 'Legal_Denomination', 'Legislation_Description', 'Min_Dry_Cocoa_Solids', 'Dry_Milk_Solids', 'MilkFat', 'Dry_Fat_Free_Cocoa_Solids', 'Typical_Chocolate_Liquor', 'Typical_Cocoa_Content', 'Total_Legal_Fat_Content', 'MilkFat_Chocolate_Part', 'Dry_Milk_Solids_On_Total_Production', 'Dry_Milk_Solids_Chocolate_Part', 'Typical_Nonfat_Milk_Solids', 'Typ_Nonfat_Cocoa_Sol_Choc_Part', 'Sum_Dry_Cocoa_And_Milk_Solids', 'Cocoa_Butter_Content', 'Alkalizing_Agent_K2CO3_DFFCS', 'Component', 'Item', 'Level', 'Material_Group', 'Material_Type', 'Parent_Material', 'Parent_Material_Label', 'Source_Generated_Field', 'components_Specifications', 'Sugars_g', 'Salt_g', 'Trans_Fatty_Acid_TFa_g', 'Energy_Value_Kcal', 'Energy_Value_Kj', 'Protein_g', 'Protein_DV_perc', 'Total_Carbohydrates_g', 'Total_Carbohydrates_DV_perc', 'Saturated_Fatty_Acid_g', 'Calories_From_Fat', 'Cholesterol_mg', 'Cholesterol_DV_perc', 'Total_Fat_DV_perc', 'Fibre_g', 'Dietary_Fibre_DV_perc', 'Vitamin_A_mcg', 'Vitamin_C_mg', 'Sodium_mg', 'Sodium_DV_perc', 'Iron_mg', 'Calcium_mg', 'Available_Carbohydrates_g', 'Total_Fat_g', 'Allergen_Statements', 'Contains_Milk_Proteins', 'Contains_Egg_Products', 'Contains_Soy_Proteins', 'Contains_Wheat', 'Contains_Rye', 'Contains_Fish', 'Contains_Crustacean_And_Shell_Fish', 'Contains_Hazelnuts_Almonds', 'Contains_Peanuts', 'Contains_Sulphite_E220_E227', 'Contains_Celery', 'Contains_Sesame_Products', 'Suitable_For_Vegetarians', 'Suitable_For_Vegans', 'Hazelnut_Oil_Almond_Oil', 'Contains_Sesame_Oil', 'Contains_Peanut_Oil', 'Contains_Mustard', 'Contains_Molluscs', 'Contains_Lupin', 'Contains_Buckwheat', 'Plant_BoM_Owner_Short', 'Underlying_Liquid', 'Dimensions_Vibration_Drops_EU_Short', 'Project_Number_Short', 'Dimensions_Production_Tools_Us', 'Material_Description', 'Base_Type', 'Moulding_Type', 'Product_Type', 'Colour_TF', 'Project_Manager', 'Dimensions_Vibration_Drops_EU', 'Project_Phase', 'Certification', 'Base_Colour', 'Additional_Colour', 'Kosher_Certificate', 'Country_Claim', 'Plant_BoM_Owner', 'Type_3_Short', 'Dosage_Per_200ml_Cold_Milk', 'Dosage_Per_200ml_Cold_Water', 'Dosage_Per_200ml_Hot_Milk', 'Dosage_Per_200Ml_Hot_Water', 'Fineness_Type', 'Colour_L_Value_From', 'pH_From', 'pH_To', 'Normalised_Linear_Mpa_S_From', 'Normalised_Linear_Viscosity_mPaS_To', 'Normalised_Casson_Mpa_S_From', 'Normalised_Casson_Mpa_S_To', 'Normalised_Yield_Pa_From', 'Normalised_Yield_Pa_To', 'Fineness_Micrometer_From', 'Fineness_Micrometer_To', 'Dimensions_Length_From', 'Dimensions_Length_To', 'Dimensions_Width_From', 'Dimensions_Width_To', 'Dimensions_Height_From', 'Dimensions_Count_Kg_From', 'Dimensions_Count_Kg_To', 'Dimensions_Sieve_Fraction_From', 'Dimensions_Sieve_Fraction_To', 'Protein', 'Dimensions_Count_lb_From', 'Dimensions_Count_lb_To', 'Brookfield_40C_S27_20_RPM_From', 'Brookfield_40C_S27_20_RPM_To', 'Brookfield_50C_S27_20_RPM_From', 'Brookfield_50C_S27_20_RPM_To', 'Brookfield_50C_S27_Regression_From', 'Brookfield_50C_S27_Regression_To', 'Brookfield_50C_S27_Yield_From', 'Brookfield_50C_S27_Yield_To', 'Brookfield_40C_S27_Regression_From', 'Brookfield_40C_S27_Regression_To', 'Brookfield_40C_S27_Yield_From', 'Brookfield_40C_S27_Yield_To', 'Water_Activity_From', 'Water_Activity_To', 'Shelflife', 'Bulk_Density_Untapped_From', 'Bulk_Density_Untapped_To', 'Bulk_Density_Tapped_x100_From', 'Bulk_Density_Tapped_x100_To', 'Dosage_Test_Grams_From', 'Dosage_Test_Grams_To', 'Material_Group_Short', 'Packaging_Info', 'Sales_Organisation', 'Plant_Where_Produced_OR_Available', 'Primary_Weight_Unit', 'Primary_Count_Unit', 'Material_Group_Long', 'Brand', 'Kosher_recipe_not_certificate_', 'Marking', 'Primary_weight', 'Primary_Count', 'NGM_Status', 'Regional_Supply_Policy_West_Europe', 'Additional_Premium_Group', 'Regional_Speciality_Category_West_Europe', 'Regional_Speciality_Category_East_Europe', 'Regional_Speciality_Category_US', 'Regional_Speciality_Category_Asia', 'Regional_Supply_Policy_US', 'Mass_Balance_Certification', 'Western_EU_BC_Selection', 'Regional_Sales_Forecast_West_Europe', 'Regional_Sales_Forecast_East_Europe', 'Regional_Sales_Forecast_US', 'Regional_Sales_Forecast_Asia', 'Regional_Premium_Category_West_Europe', 'Regional_Premium_Category_East_Europe', 'Regional_Premium_Category_US', 'Regional_Premium_Category_Asia', 'Product_Category_West_Europe_', 'Product_Category_East_Europe', 'Product_Category_US', 'Product_Category_Asia', 'Eastern_EU_BC_Selection', 'Standard_Range_Mexico', 'Standard_Range_US', 'Commercial_Name', 'Commodity_Code', 'Lifecycle_status', 'Distribution_Channel', 'Calculated_Price_Currency', 'Sales_Organisation_Distribution_Channel', 'Delivery_Unit_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_Sales_Org_Dc_Qty_In_UoM', 'Minimum_Order_Quantity_In_UoM', 'Delivery_unit_Qty_In_UoM', 'Replenishment_Lead_Time', 'Sales_Last_12_Months_North_America_in_KG', 'Sales_Last_12_Months_Asia_in_KG_', 'Sales_Last_12_Months_EEMEA_in_KG_', 'Sales_Last_12_Months_West_Europe_in_KG', 'Sales_Last_12_Months_South_America_in_KG', 'Calculated_Price', 'Sales_Last_12_Months_Total_In_Kg', 'Contains_Hydrogenated', 'Fat', 'Polyols', 'Nuts_and_Almonds', 'Total_Fat_On_Spec_perc_From', 'Total_Fat_On_Spec_perc_To', 'Hydrogenated', 'Core_OR_Extended', 'Core_Region', 'Core_Country', 'Core_Segment', 'Core_Subsegment', 'Category', 'Region', 'Customer_Dedication', 'Proj_Phase', 'Pack_Code', 'Base_Unit_Of_Measure', 'Smallest_Unit_Weight_In_Kg', 'Sample_Unit', 'Units_Per_Layer', 'Units_Per_Pallet', 'Pallet', 'Pallet_Net_Weight_In_Kg', 'Pallet_Gross_Weight_In_Kg', 'Length', 'Width', 'Height', 'Pallet_Type', 'Sales_Unit', 'Layer', 'Delivery_Unit_AUM', 'Certification_Tag', 'Colour_Type_Tag', 'Flavor_Type_Tag', 'Ingredients_Tag']\n"
     ]
    }
   ],
   "source": [
    "print(list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "print(len(list(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
